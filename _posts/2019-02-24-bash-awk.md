---
title: "AWK Loops"
excerpt: Struggling with writing a loop in AWK. 
categories: tech
thumbnail: wrench
published: true
---

![awk](/images/awk.png)

I was trying to collate a series of .csv log files that are named by date (e.g., 2019-02-24.csv). These are daily files, and there are a bunch of them, so it was going to take me a long while to do, and it'd have to be done again sometime in the future - maybe on a daily basis. It would be better to have a repeatable (i.e., scripted) process. To that end, I crafted an AWK script to do my dirty work. 

AWK - derived from the last names of its inventors; Alfred Aho, Peter Weinberger, and Brian Kernighan - is a data extraction and text processing programming language that dates back to the 1970s. It is bundled with most *nix packages in one variety or another and is often closely associated with sed (a stream [text] editor) and grep (globally search a regular expression and print), but is more powerful due to the fact that it is an actual language whereas the others are utilities. All three of them have been around for a while and there's a wealth of knowledge out there for those who seek it out (hint: [Stack Overflow](https://stackoverflow.com/) has tons of posts, probably one covering what you are trying to do!). 

So, here is where I landed with the task I wanted to accomplish: 

```
awk 'FNR==1 && NR!=1{next;}{print}' 201[8-9]-[0-1][0-2]-[0-3][0-9].csv >> history.csv
```
There are some interesting component to what this deceptively small script does. The analog version of my workflow looks like this:

Every daily file has hundreds, or even thousands, of entries that need to be copied and appended to a single log. Manually the process is to open each file, copy everything except the first row (which contains the headers, or column titles), and paste the contents onto the bottom of the single log. This doesn't sound like much, but when you start scrolling through thousands, tens of thousands, or hundreds of thousands of lines of text to get where you're going... Yeah! It can be cumbersome. 

Breaking the process down into chunks helps to simplify what needs to be done. The first portion of my script determines which part of the daily log has to be copied and pasted into the master log:

```
'FNR==1 && NR!=1{next;}{print}'
```
It's basically saying, "if we haven't read the first row somewhere else before, then include it, if we've already done that, skip over it and get everything else. Using this allows me to have one set of headers (column titles) for the resultant master log; that is important because we could wind up with weird results later on down the road if the headers are plastered willy-nilly througout the document. 

The next part determines which files we will ingest into the process: 

```
201[8-9]-[0-1][0-2]-[0-3][0-9].csv
```
The numbers in the brackets represent ranges from which we can select parts and pieces to make up the filenames of the daily files in our folder. For example, the first set of brackets ```[8-9]``` allows us to use any integer (whole number) in the range of eight to nine. There are only two possibilities, eight or nine, and it will select the lowest one first. When added to the text/numbers prior to the opening bracket ```201``` we wind up with either ```2018``` or ```2019```. 

The next four sets of brackets detail the options for the month and day designators - 01 through 12 for the month - 01 through 31 for the day (technically they both start with 00, but we won't see that unless there is a misnamed daily file in the folder - then there will be some weirdness going on! Perhaps I should build in a check to make sure that I don't import any weirdly named files, but as it is, even if we did, the worst thing that could happen is that we'd have some out-of-sequence log entries in the master log).  

```
>> history.csv
```
The final piece does the magic of appending each daily file to the master log. 

Armed with this script, I can pop into the folder where I keep all my daily logs and, in a matter of seconds, create a master log with all the events that I've been capturing for any number of days, weeks, months, or years that I want to analyze. This saves a huge amount of time and allows me to focus on the bigger picture while being informed of all the details that comprise it. 

<hr>

When I finalized the script above, it dawned on me that I might just want to add a single file to the master log, rather than running through the entire process again every time I want to update the contents (even though that takes less than a minute to do). So I reached into my little bag of tricks (it's a really LITTLE bag) and pulled out part of the script that I started composing earlier in the day. A little tweaking, and I came up with one that works for a single file at a time: 

```
awk '!/"_time",SECTION1,SECTION2,SECTION3/' 2019-02-24.csv >> history.csv
```

Voila! 

